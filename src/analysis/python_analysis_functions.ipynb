{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "from holoviews import opts,dim\n",
    "import matplotlib as plt \n",
    "import colorcet as cc\n",
    "import functools as ft\n",
    "import operator as ops \n",
    "import os \n",
    "import json \n",
    "\n",
    "pn.extension()\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# crypto analysis using python\n",
    "datadir = \"crypto_market_data_1\"\n",
    "cachedir = \"cache\"\n",
    "folders = os.listdir(datadir)\n",
    "\n",
    "# get the full fname of a specific file \n",
    "def get_fname(day,pair,futures_or_spot, ftype) : \n",
    "         return \"{}/{}/{}/{}/{}_{}.json\".format(datadir,day,pair,futures_or_spot,pair,ftype)\n",
    "\n",
    "# read a text file \n",
    "def read_text_file(fname) : \n",
    "    with open(fname, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_dictionary_string(s) : \n",
    "    d = json.loads(s) \n",
    "    r = {} \n",
    "    r['p'] = float(d['p'])\n",
    "    r['q'] = float(d['q'])\n",
    "    r['m'] = d['m']\n",
    "    r['epoch_time_ms'] = float(d['T'])\n",
    "    return r \n",
    "\n",
    "def extract_data_from_file(day,pair,futures_or_spot, ftype) :  \n",
    "    # get the text of the file \n",
    "    fstr = read_text_file(get_fname(day,pair,futures_or_spot, ftype)) \n",
    "    # parse it into a pandas dataframe \n",
    "    data =  pd.DataFrame([ read_dictionary_string(x) for x in fstr.split(\"\\n\") if x != \"\" ])    \n",
    "    # convert the epoch times (currently floats) into timestamps , then return \n",
    "    return data.assign(tms = pd.to_datetime(data['epoch_time_ms'], unit='ms'))\n",
    "\n",
    "def extract_data_from_file_with_tbins(day,pair,futures_or_spot, ftype, aggregate=\"T\") :  \n",
    "    data = extract_data_from_file(day,pair,futures_or_spot, ftype) \n",
    "    data = data.assign(tbin= lambda x: x['tms'].round(aggregate))\n",
    "    return data \n",
    "\n",
    "def trade_aggregator(df) :  \n",
    "    \"\"\"\n",
    "    Function that aggregates trades over a given interval. It does not check the interval kength - it only summarizes the data that is within each group \n",
    "    Will likely keep contributing summary statistics over time as this modular approach allows for maintainability \n",
    "    \"\"\"\n",
    "    \n",
    "    p = df['p']\n",
    "    p_avg = np.mean(p) \n",
    "    p_max = np.max(p)\n",
    "    p_min = np.min(p)\n",
    "    p_range = p_max - p_min\n",
    "    p_open = df.iloc[0].p\n",
    "    p_close = df.iloc[-1].p \n",
    "    p_change = p_close - p_open \n",
    "    p_change_percent = 100*p_change/p_open \n",
    "    p_range_percent  = p_range/p_open #not sure if denom should be open or min \n",
    "    \n",
    "    df_sell = df[df['m']== True]['q'] # \"buyer is market maker\"\n",
    "    v_sell_tot  = np.sum(df_sell)\n",
    "    v_sell_max  = np.max(df_sell)\n",
    "    \n",
    "    df_buy = df[df['m']== False]['q'] \n",
    "    v_buy_tot   = np.sum(df_buy)\n",
    "    v_buy_max  = np.max(df_buy)\n",
    "\n",
    "    v_tot = np.sum(df['q']) \n",
    "    # (above) total volume  - not sure how meaningful this really is--- interesting \n",
    "    # am starting to think about resistance metrics again --  \n",
    "    \n",
    "    v_net = v_buy_tot - v_sell_tot \n",
    "    \n",
    "    return pd.DataFrame([{\n",
    "        'p_avg' : p_avg, \n",
    "        'p_max' : p_max, \n",
    "        'p_min' : p_min, \n",
    "        'p_range' : p_range, \n",
    "        'p_open' : p_open, \n",
    "        'p_close' : p_close, \n",
    "        'p_change' : p_change, \n",
    "        'p_change_percent' : p_change_percent, \n",
    "        'p_range_percent' : p_range_percent, \n",
    "        'v_sell_tot' : v_sell_tot , \n",
    "        'v_sell_max' : v_sell_max,  \n",
    "        'v_buy_tot' : v_buy_tot, \n",
    "        'v_buy_max' : v_buy_max ,\n",
    "        'v_tot' : v_tot , \n",
    "        'v_net' : v_net, \n",
    "    }])\n",
    "\n",
    "def extract_tbinned_data_from_file(day,pair,futures_or_spot, ftype, aggregate=\"T\") : \n",
    "    \n",
    "    # this function will load aggregated and summarized data, and if it has already been loaded then it will returned the cached version\n",
    "    # this is good, however if the 'trade_aggregator' function is changed then the cache will incorrectly return an old value \n",
    "    \n",
    "    # will have to think how to mitigate this \n",
    "    \n",
    "    # I think i will start by employing a direct caching technique right here \n",
    "    # each unique access will be defined by the access_id\n",
    "    access_id = \"_\".join([day,pair,futures_or_spot,ftype,aggregate])  + \".pkl\"\n",
    "    access_namespace = \"extract_tbinned_data_from_file\"  \n",
    "    \n",
    "    dirname = os.path.join(cachedir,access_namespace) \n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "    fname = os.path.join(dirname,access_id)\n",
    "    print(\"cache req: {}\".format(fname)) \n",
    "        \n",
    "    if os.path.exists(fname) : \n",
    "        # if the file exists this is considered a cache hit \n",
    "        print(\"cache hit!\")\n",
    "        # parse the file and get the data \n",
    "        data = pd.read_pickle(fname) \n",
    "        # note that even though we are reading the data from disk this should be much faster as it is a \n",
    "        # binary stored formate of aggregated trades, rather than the raw format of strings reprenting individual trades as below \n",
    "        return data \n",
    "     \n",
    "    # if we are here then its a cache miss ... \n",
    "    print(\"cache miss! ~> proceeding with request \")\n",
    "    # extract data from the original text file format \n",
    "    data =  extract_data_from_file_with_tbins(day,pair,futures_or_spot, ftype, aggregate=\"T\")\n",
    "    data_tbinned = data.groupby(['tbin']).apply(trade_aggregator).reset_index()\n",
    "    \n",
    "    # can add additional information to the dataframe here \n",
    "    # cummulative sums over the day \n",
    "    data_tbinned = data_tbinned.assign(v_buy_cumulative=data_tbinned['v_buy_tot'].cumsum())\n",
    "    data_tbinned = data_tbinned.assign(v_sell_cumulative=data_tbinned['v_sell_tot'].cumsum())\n",
    "    data_tbinned = data_tbinned.assign(v_tot_cumulative=data_tbinned['v_tot'].cumsum()) \n",
    "    \n",
    "    # and finally we have to store the cached data prior to returning \n",
    "    data_tbinned.to_pickle(fname)\n",
    "    print(\"cache: wrote: {}\".format(fname)) \n",
    "    \n",
    "    return data_tbinned \n",
    "\n",
    "def get_future_spot_compared_data(day,pair,aggregate='T') : \n",
    "    print(\"Extracting futures data\")\n",
    "    f = extract_tbinned_data_from_file(day,pair,\"FUTURES\",\"TRADE\", aggregate) \n",
    "    print(\"Extracting spot data\")\n",
    "    s = extract_tbinned_data_from_file(day,pair,\"SPOT\",\"TRADE\", aggregate) \n",
    "    \n",
    "    p_avg_diff = f['p_avg'] - s['p_avg']  \n",
    "    p_mean = (f['p_avg'] + s['p_avg'])/2 \n",
    "    p_avg_diff_percent = 100*p_avg_diff/p_mean \n",
    "    compared = pd.DataFrame({ \n",
    "        'tbin' : f['tbin'] , \n",
    "        'p_avg_diff' : p_avg_diff, \n",
    "        'p_avg_diff_percent' : p_avg_diff_percent, \n",
    "        'v_tot_diff' : f['v_tot']  - s['v_tot'] , \n",
    "        'v_sell_tot_diff' : f['v_sell_tot'] - s['v_sell_tot'] , \n",
    "        'v_buy_tot_biff' : f['v_buy_tot'] - s['v_buy_tot'] , \n",
    "        'p_percent_diff' : f['p_change_percent'] - s['p_change_percent'], \n",
    "        'v_net_diff' : f['v_net'] - s['v_net'] , \n",
    "    })\n",
    "    return (f,s,compared) \n",
    "\n",
    "def convert_future_spot_compare_to_ds(f,s,c) :  \n",
    "    c_ds = hv.Dataset(c, kdims=['tbin'],vdims=['p_avg_diff','p_avg_diff_percent','v_tot_diff','p_percent_diff','v_net_diff'])\n",
    "    f_ds = hv.Dataset(f, kdims=['tbin'], vdims=['p_avg','p_change', 'p_change_percent', 'v_sell_tot', 'v_buy_tot' , 'v_tot','v_net'])\n",
    "    s_ds = hv.Dataset(s, kdims=['tbin'], vdims=['p_avg','p_change', 'p_change_percent', 'v_sell_tot', 'v_buy_tot' , 'v_tot','v_net'])\n",
    "    return (f_ds,s_ds,c_ds)    \n",
    "\n",
    "def rsz(graph,x) : \n",
    "        return graph.options(width=x[0],height=x[1])\n",
    "\n",
    "default_width = 1300 \n",
    "\n",
    "def analyze_future_spot(day,pair,aggregate=\"T\") : \n",
    "    (f, s, c)         = get_future_spot_compared_data(day,pair,aggregate) # get the data \n",
    "    (f_ds, s_ds, c_ds )  = convert_future_spot_compare_to_ds(f,s,c) # convert it to hv objects \n",
    "\n",
    "    # time to visualize now \n",
    "    l1 = hv.Curve(c_ds,'tbin','p_avg_diff_percent',)\n",
    "    l2 = hv.Curve(f_ds,'tbin', 'v_net',label='futures') * hv.Curve(s_ds,'tbin','v_net',label='spot')\n",
    "    #l3 = hv.Curve(f_ds,'tbin','v_sell_tot') \n",
    "    graphs = [l1,l2]\n",
    "\n",
    "    s1 = f_ds.to(hv.Curve,'tbin','p_avg',label='futures') \n",
    "    s2 = s_ds.to(hv.Curve,'tbin','p_avg', label='spot')\n",
    "    price_graph = ft.reduce(ops.mul, [rsz(x,(default_width,400)) for x in [s1,s2]] )\n",
    "   \n",
    "    diff_graphs = ft.reduce(ops.add, [rsz(x,(default_width,200)) for x in graphs] ).cols(1)  \n",
    "    return ((price_graph, diff_graphs), f, s, c, f_ds,s_ds,c_ds )\n",
    "\n",
    "def compare_futures_spot_v_vs_dp(f_ds,s_ds) : \n",
    "    # futures \n",
    "    s1 = hv.Scatter(f_ds,'v_net','p_change_percent',label='futures')\n",
    "    s2 = hv.Scatter(s_ds,'v_net','p_change_percent',label='spot')\n",
    "    gs = [s1,s2]\n",
    "    return ft.reduce(ops.add, [rsz(x,(int(default_width/2),500)) for x in gs]).cols(2)\n",
    "\n",
    "def epoch_graph_for_df(s,label=None) :\n",
    "    s['epoch'] = s['tbin'].apply(lambda x:x.value) \n",
    "    s['index'] = np.arange(len(s))\n",
    "\n",
    "    new_ds = hv.Dataset(s,kdims=[\"index\"],vdims=[\"epoch\"]) \n",
    "    g = rsz(hv.Curve(new_ds,'index','epoch',label=label), (default_width,300)) \n",
    "    return (g,new_ds) \n",
    "\n",
    "\n",
    "def futures_spot_epoch_graph(f,s) : \n",
    "    (sg,snds) = epoch_graph_for_df(s,\"spot\")\n",
    "    (fg,fnds) = epoch_graph_for_df(f,\"futures\") \n",
    "    g = sg*fg\n",
    "    return (g,fg,fnds,sg,snds) \n",
    "    \n",
    "    \n",
    "def futures_spot_total_v(f_ds,s_ds)     : \n",
    "    sz=(default_width,400)\n",
    "    return rsz(f_ds.to(hv.Curve,'tbin', 'v_tot',label='futures'),sz) *  rsz(s_ds.to(hv.Curve,'tbin', 'v_tot',label='spot'),sz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
